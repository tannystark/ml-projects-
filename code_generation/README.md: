# Code Generation Model using Fine-Tuned Large Language Models (LLMs)

## Overview
Built a full Code Generation system by fine-tuning a pre-trained Large Language Model (Salesforce CodeGen-350M-mono) on a custom Python dataset scraped from GitHub repositories.

## Approach
- Collected Python code snippets using GitHub API (PyGithub).
- Extracted function definitions to create a HuggingFace-compatible dataset.
- Fine-tuned a pre-trained LLM using HuggingFace's Trainer API.
- Designed a custom prompt generation and code decoding pipeline.
- Tested model outputs manually to validate correctness.

## Technologies Used
- Python
- GitHub API (PyGithub)
- HuggingFace Transformers
- HuggingFace Datasets
- PyTorch

## Results
- Fine-tuned model generated cleaner, more accurate Python code from natural language prompts.
- Reduced syntax and logical errors compared to the original pre-trained model.

## Key Learnings
- Full lifecycle experience: data collection, preprocessing, fine-tuning, evaluation.
- Fine-tuning improves model specialization on domain-specific tasks.
- Prompt engineering plays a critical role in guiding LLM outputs.

## Future Work
- Extend dataset with code from multiple domains (ML, Web Dev, APIs).
- Automate output testing using runtime evaluations.
- Experiment with larger models and RLHF (Reinforcement Learning from Human Feedback).


